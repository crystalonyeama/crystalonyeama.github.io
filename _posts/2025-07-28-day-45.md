---
layout: post
title: "Day 45 – Model Tuning and Architecture Documentation"
date: 2025-07-28
author: Crystal Onyeama
permalink: /day45.html
tags: ["model training", "DCAN", "XGBoost", "ResNet50", "EfficientNetB0", "research paper", "model architecture"]

what_i_learned: |
  Today we spent most of our time rerunning the model and making targeted adjustments in an effort to improve its accuracy. We focused on fine-tuning parameters and ensuring the training process remained stable and reproducible across runs.

  In addition to model tuning, I dedicated a large portion of the day to working on our research paper—specifically, the section outlining our model architecture. I explained the structure and reasoning behind each model we used: DCAN (for its dual attention and multimodal fusion), XGBoost (for feature ranking and tabular classification), ResNet50 (for deep feature extraction via residual connections), and EfficientNetB0 (for its compound scaling and efficiency). Writing about these models helped me better understand their individual strengths and how they contribute to our overall system.

blockers: |
  No major blockers today, though some runs of the model took longer than expected as we worked through tuning and adjustments.

reflection: |
  Diving deeper into our architecture made me appreciate how much thought has gone into combining these models. Each one brings something unique to the table, and documenting their design clarified how they all work together to support accurate skin lesion classification.
---
