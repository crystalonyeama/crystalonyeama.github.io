---
layout: post
title: "Day 32 – XGBoost Data Preprocessing & Model Training"
date: 2025-07-09
author: Crystal Onyeama
permalink: /day32.html
tags: ["data cleaning", "XGBoost", "model training", "feature selection", "Python"]

what_i_learned: |
  Today was a direct continuation from yesterday as we continued working with the “Hospital Italiano” dataset in preparation for XGBoost modeling. Our mentor, Blessing, gave us a structured 6-step workflow to follow for cleaning, preparing, and modeling the data. The process really helped break the work into manageable pieces and ensured we were covering all the critical components for a strong machine learning pipeline.

  We began with Step 1: loading and exploring the data. This included understanding the structure of the dataset, scanning for missing values, and identifying patterns in our columns. In Step 2, we prepped the target column by converting the `benign_malignant` values into binary numeric format (e.g., 0 for benign, 1 for malignant).

  For Step 3, we selected a core set of features to include in our model: `age_approx`, `sex`, `anatom_site_general`, `fitzpatrick_skin_type`, and `dermoscopic_type`. We cleaned and encoded these columns to make them compatible with machine learning algorithms. In Step 4, we split the dataset using `train_test_split` to create training and testing subsets. Next, we used `XGBClassifier` from the XGBoost library to train our model in Step 5.

  Step 6 involved evaluating the model's performance using metrics from `sklearn.metrics`, helping us see how well our classifier performed. As a bonus (Step 7), we generated a feature importance chart to visually interpret which variables were most influential in predicting the target label.

blockers: |
  Most of the work went smoothly, but we had to pause a few times to make sure each feature was properly cleaned and encoded—especially with categorical data like `sex` and `anatom_site_general`. Debugging during model training also took extra attention, particularly when checking for missing values.

reflection: |
  This structured process helped demystify what can feel like a complex modeling task. It was especially useful to see the impact of features like `fitzpatrick_skin_type` and `age_approx`, which appeared to carry strong predictive power. I feel more confident now in building and interpreting machine learning models with real-world health data.
---
